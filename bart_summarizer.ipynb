{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import random\n",
    "from evaluate import load\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "import kagglehub\n",
    "import evaluate\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.8), please consider upgrading to the latest version (0.3.12).\n",
      "Path to dataset files: /Users/milez/.cache/kagglehub/datasets/snap/amazon-fine-food-reviews/versions/2\n",
      "/Users/milez/.cache/kagglehub/datasets/snap/amazon-fine-food-reviews/versions/2/database.sqlite\n",
      "/Users/milez/.cache/kagglehub/datasets/snap/amazon-fine-food-reviews/versions/2/Reviews.csv\n",
      "/Users/milez/.cache/kagglehub/datasets/snap/amazon-fine-food-reviews/versions/2/hashes.txt\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"snap/amazon-fine-food-reviews\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "for dirname, _, filenames in os.walk('/Users/milez/.cache/kagglehub/datasets/snap/amazon-fine-food-reviews/versions/2'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/milez/.cache/kagglehub/datasets/snap/amazon-fine-food-reviews/versions/2/\" + \"reviews.csv\", usecols=[\"Id\",\"Summary\", \"Text\", \"ProductId\"])\n",
    "df.dropna(subset=[\"Summary\", \"Text\"], inplace=True)\n",
    "df = df.sample(10000, random_state=42)\n",
    "df = df.rename(columns={\"Summary\": \"target_text\", \"Text\": \"input_text\"})\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(len(dataset))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_balanced_reviews(csv_path):\n",
    "    df = pd.read_csv(csv_path, usecols=[\"Id\",\"Score\",\"Summary\", \"Text\", \"ProductId\"])\n",
    "    # Filter for 1, 2, and 5 star reviews\n",
    "    low = df[df.Score.isin([1,2])]\n",
    "\n",
    "    high = df[df['Score'] == 5]\n",
    "\n",
    "\n",
    "    #n_neg = len(negative)\n",
    "    #target_pos = n_neg  \n",
    "    \n",
    "    #positive_down = positive.sample(n=target_pos, random_state=42, replace=False)\n",
    "    \n",
    "    #balanced = pd.concat([negative, positive_down]) \\\n",
    "                 #.sample(frac=1, random_state=42) \\\n",
    "                 #.reset_index(drop=True)\n",
    "    #return balanced\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    n_samples = min(len(low), len(high))\n",
    "    low_balanced = low.sample(n=n_samples, random_state=42)\n",
    "    high_balanced = high.sample(n=n_samples, random_state=42)\n",
    "    balanced_df = pd.concat([low_balanced, high_balanced]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    balanced_df.to_csv(\"FilteredReviews.csv\", index=False)\n",
    "    return balanced_df\n",
    "\n",
    "# Calls the function\n",
    "balanced_reviews = load_balanced_reviews(\"/Users/milez/.cache/kagglehub/datasets/snap/amazon-fine-food-reviews/versions/2/\" + \"reviews.csv\")\n",
    "balanced_reviews.dropna(subset=[\"Summary\", \"Text\"], inplace=True)\n",
    "balanced_reviews = balanced_reviews.sample(10000, random_state=42)\n",
    "balanced_reviews = balanced_reviews.rename(columns={\"Summary\": \"target_text\", \"Text\": \"input_text\"})\n",
    "dataset1 = Dataset.from_pandas(balanced_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(\"lucadiliello/bart-small\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"lucadiliello/bart-small\")\n",
    "\n",
    "print(len(dataset))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    ) | {\n",
    "        \"labels\": tokenizer(\n",
    "            examples[\"target_text\"],\n",
    "            max_length=64,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )[\"input_ids\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575cbc1eb48640f89ba250c754bd554b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split[\"train\"]\n",
    "eval_dataset = split[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train = train_dataset#.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/milez/anaconda3/envs/nlp3/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/fv/_1h3qgh10y31djr_b1ns76km0000gn/T/ipykernel_41821/938406448.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18000' max='18000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18000/18000 1:38:17, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.939000</td>\n",
       "      <td>0.892749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.564100</td>\n",
       "      <td>0.530795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.519000</td>\n",
       "      <td>0.522671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.517200</td>\n",
       "      <td>0.519291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/milez/anaconda3/envs/nlp3/lib/python3.10/site-packages/transformers/modeling_utils.py:3353: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./bart_summarizer/tokenizer_config.json',\n",
       " './bart_summarizer/special_tokens_map.json',\n",
       " './bart_summarizer/vocab.json',\n",
       " './bart_summarizer/merges.txt',\n",
       " './bart_summarizer/added_tokens.json')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir= \"./bart_summarizer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir= \"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./bart_summarizer\")\n",
    "tokenizer.save_pretrained(\"./bart_summarizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/milez/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/milez/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/milez/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")  \n",
    "meteor = evaluate.load(\"meteor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "initial_model = BartForConditionalGeneration.from_pretrained(\"lucadiliello/bart-small\")\n",
    "\n",
    "finetuned_model = BartForConditionalGeneration.from_pretrained(\"./bart_summarizer3\")\n",
    "finetuned_tokenizer = BartTokenizer.from_pretrained(\"./bart_summarizer3\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Model Metrics:\n",
      "ROUGE-L (Initial pre-trained model): 0.0681\n",
      "BLEU (Initial pre-trained model): 0.0068\n",
      "METEOR (Initial pre-trained model): 0.1373\n",
      "\n",
      "Fine-tuned Model Metrics:\n",
      "ROUGE-L (Fine-tuned model): 0.0645\n",
      "BLEU (Fine-tuned model): 0.0102\n",
      "METEOR (Fine-tuned model): 0.0417\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model_to_eval, dataset, tokenizer, max_input_length=512, max_target_length=64):\n",
    "    model_to_eval.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for example in dataset.select(range(200)):\n",
    "        input_text = \"summarize: \" + example[\"input_text\"]\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True, max_length=max_input_length)\n",
    "        input_ids = input_ids.to(model_to_eval.device)\n",
    "        with torch.no_grad():\n",
    "            output_ids = model_to_eval.generate(input_ids, max_length=max_target_length)\n",
    "        pred = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        predictions.append(pred)\n",
    "        references.append(example[\"target_text\"])\n",
    "    \n",
    "    results = {}\n",
    "    # ROUGE-L\n",
    "    rouge_results = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "    results[\"rougeL\"] = rouge_results[\"rougeL\"]\n",
    "    # BLEU score\n",
    "    bleu_results = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    results[\"bleu\"] = bleu_results[\"bleu\"]\n",
    "    # METEOR score\n",
    "    meteor_results = meteor.compute(predictions=predictions, references=references)\n",
    "    results[\"meteor\"] = meteor_results[\"meteor\"]\n",
    "\n",
    "    return results\n",
    "\n",
    "# Initial model\n",
    "initial_results = evaluate_model(initial_model, eval_dataset, tokenizer)\n",
    "print(\"Initial Model Metrics:\")\n",
    "print(f\"ROUGE-L (Initial pre-trained model): {initial_results['rougeL']:.4f}\")\n",
    "print(f\"BLEU (Initial pre-trained model): {initial_results['bleu']:.4f}\")\n",
    "print(f\"METEOR (Initial pre-trained model): {initial_results['meteor']:.4f}\")\n",
    "\n",
    "# Fine-tuned model\n",
    "finetuned_results = evaluate_model(finetuned_model, eval_dataset, tokenizer)\n",
    "print(\"\\nFine-tuned Model Metrics:\")\n",
    "print(f\"ROUGE-L (Fine-tuned model): {finetuned_results['rougeL']:.4f}\")\n",
    "print(f\"BLEU (Fine-tuned model): {finetuned_results['bleu']:.4f}\")\n",
    "print(f\"METEOR (Fine-tuned model): {finetuned_results['meteor']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: Great flavor\n"
     ]
    }
   ],
   "source": [
    "# Load your fine-tuned model and tokenizer\n",
    "#tokenizer = BartTokenizer.from_pretrained(\"path_to_your_fine_tuned_model\")\n",
    "#model = BartForConditionalGeneration.from_pretrained(\"path_to_your_fine_tuned_model\")\n",
    "\n",
    "# Sample input text\n",
    "#input_text = \"This Item Taste Like Dirt.. I've Prob Used it 4 Times & Now It's Just Sitting in MY Freezer.. I Have A High Tolerance for Nasty Stuff.. Just Don't Really Like this Product.. Something In Grinding It Up Makes It Taste Nasty.. The Hulled Seeds Nutiva Sells Are Way Better.. If You Want Good Tasting Hemp Protein Powder It's $15/lb @ Earthshiftproducts.com but It Taste Wayyy Better Actually Taste Good From Earthshift..\"\n",
    "#input_text = \"This compact wireless speaker delivers excellent sound quality with impressive battery life.\"\n",
    "input_text =\"These cinnamon bears have great flavor and do not taste sugar free.  My only issue is that they should be softer.\"\n",
    "# Tokenize the input\n",
    "inputs = finetuned_tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate summary\n",
    "summary_ids = finetuned_model.generate(inputs, max_length=64, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Decode and print the summary\n",
    "summary = finetuned_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_outputs(initial_model, finetuned_model, dataset, tokenizer, device, sample_size=5, max_input_length=512, max_target_length=64):\n",
    "\n",
    "    indices = list(range(len(dataset)))\n",
    "    random_indices = random.sample(indices, sample_size)\n",
    "    \n",
    "    for idx in random_indices:\n",
    "        example = dataset[idx]\n",
    "        input_str = \"summarize: \" + example[\"input_text\"]\n",
    "        input_ids = tokenizer.encode(input_str, return_tensors=\"pt\",\n",
    "                                     truncation=True, max_length=max_input_length).to(device)\n",
    "        \n",
    "        # Initial model\n",
    "        with torch.no_grad():\n",
    "            initial_output_ids = initial_model.generate(input_ids, max_length=max_target_length)\n",
    "        initial_output = tokenizer.decode(initial_output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Fine-tuned model\n",
    "        with torch.no_grad():\n",
    "            finetuned_output_ids = finetuned_model.generate(input_ids, max_length=max_target_length)\n",
    "        finetuned_output = tokenizer.decode(finetuned_output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Display the outputs\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Example ID: {idx}\")\n",
    "        print(\"Input Text:\")\n",
    "        print(example[\"input_text\"])\n",
    "        print(\"\\nReference Summary:\")\n",
    "        print(example[\"target_text\"])\n",
    "        print(\"\\nInitial Model Output:\")\n",
    "        print(initial_output)\n",
    "        print(\"\\nFine-tuned Model Output:\")\n",
    "        print(finetuned_output)\n",
    "        print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#sample_outputs(initial_model, model, eval_dataset, tokenizer, device, sample_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Id', 'ProductId', 'target_text', 'input_text', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "\n",
      "Input: summarize: Best peanut brittle ever!  However, I was disappointed in the amount of loose peanuts and candy material in the bottom of the box.  In addition, the shipping cost doubled the cost of my order; outrageous!<br />Donald\n",
      "Generated Summary: Best peanut brittle ever!\n",
      "\n",
      "Input: summarize: Nutritionists recommend that you get a serving of vegetables with every meal, but how to accomplish that with a breakfast has always been a tough one for me since I don't like starchy vegetables like potatoes, and aside from cutting up a tomato, few other things go well with yogurt, egg whites, oatmeal, or any of the other stuff I like in the morning. A simple glass of V8 works perfectly. I've always loved V8 juice for that reason, but found it just a tad too salty. The low sodium version has made V8 perfect in my opinion. It has more flavor than most tomato juices, with a nice hint of celery seed. (It also makes a mean Bloody Mary!).<br /><br />I've only given this particular version 3 stars because the shipping costs are not worth it. With the many PRIME eligible options available for V8, save yourself some cash and get one that ships for free.\n",
      "Generated Summary: Best V8 on the market!\n",
      "\n",
      "Input: summarize: I love wasabi....and these almonds are AWESOME!  They taste better than any other \"flavor enhanced\" almonds on the market.\n",
      "Generated Summary: YUM!\n",
      "\n",
      "Input: summarize: These cinnamon bears have great flavor and do not taste sugar free.  My only issue is that they should be softer.\n",
      "Generated Summary: summarize\n",
      "\n",
      "Input: summarize: Just had my third full size pie and I just have to say these make some of the best pie filling's anywhere ever made.<br /><br />  Ray\n",
      "Generated Summary: summarize\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(eval_dataset)\n",
    "indices = list(range(len(eval_dataset)))\n",
    "random_indices = random.sample(indices, 5)\n",
    "\n",
    "for idx in random_indices:\n",
    "    example = eval_dataset[idx]  \n",
    "\n",
    "    # build the prompt string\n",
    "    input_str = \"summarize: \" + example[\"input_text\"]\n",
    "    print(\"\\nInput:\", input_str)\n",
    "\n",
    "    inputs = finetuned_tokenizer(\n",
    "        input_str,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # 5) generate and decode the first element\n",
    "    summary_ids = finetuned_model.generate(\n",
    "        **inputs,\n",
    "        max_length=64,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    summary = finetuned_tokenizer.decode(\n",
    "        summary_ids[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(\"Generated Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
