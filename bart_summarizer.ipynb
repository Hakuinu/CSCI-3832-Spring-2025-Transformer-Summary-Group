{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import random\n",
    "from evaluate import load\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "import kagglehub\n",
    "import evaluate\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.8), please consider upgrading to the latest version (0.3.11).\n",
      "Path to dataset files: /Users/milez/.cache/kagglehub/datasets/snap/amazon-fine-food-reviews/versions/2\n",
      "/Users/milez/.cache/kagglehub/datasets/snap/amazon-fine-food-reviews/versions/2/database.sqlite\n",
      "/Users/milez/.cache/kagglehub/datasets/snap/amazon-fine-food-reviews/versions/2/Reviews.csv\n",
      "/Users/milez/.cache/kagglehub/datasets/snap/amazon-fine-food-reviews/versions/2/hashes.txt\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"snap/amazon-fine-food-reviews\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "for dirname, _, filenames in os.walk('/Users/milez/.cache/kagglehub/datasets/snap/amazon-fine-food-reviews/versions/2'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/milez/.cache/kagglehub/datasets/snap/amazon-fine-food-reviews/versions/2/\" + \"reviews.csv\", usecols=[\"Id\",\"Summary\", \"Text\", \"ProductId\"])\n",
    "df.dropna(subset=[\"Summary\", \"Text\"], inplace=True)\n",
    "df = df.sample(10000, random_state=42)\n",
    "df = df.rename(columns={\"Summary\": \"target_text\", \"Text\": \"input_text\"})\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(\"lucadiliello/bart-small\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"lucadiliello/bart-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    ) | {\n",
    "        \"labels\": tokenizer(\n",
    "            examples[\"target_text\"],\n",
    "            max_length=64,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )[\"input_ids\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c09b72f1574f3594b09941ba0c546f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split[\"train\"]\n",
    "eval_dataset = split[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train = tokenized_dataset.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fv/_1h3qgh10y31djr_b1ns76km0000gn/T/ipykernel_26451/445167750.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 08:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.284000</td>\n",
       "      <td>2.141588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.128000</td>\n",
       "      <td>2.394907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.118600</td>\n",
       "      <td>2.279571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/milez/anaconda3/envs/nlp3/lib/python3.10/site-packages/transformers/modeling_utils.py:3353: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./bart_summarizer/tokenizer_config.json',\n",
       " './bart_summarizer/special_tokens_map.json',\n",
       " './bart_summarizer/vocab.json',\n",
       " './bart_summarizer/merges.txt',\n",
       " './bart_summarizer/added_tokens.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir= \"./bart_summarizer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir= \"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./bart_summarizer\")\n",
    "tokenizer.save_pretrained(\"./bart_summarizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e305422abf457a96177d76c8b3344c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fd0df3b1bb40a09e3562129cc79f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfc6dd6fd4b4789a1dca23aca2ee700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e7866585524b6aa806196c1f2c62af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/milez/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/milez/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/milez/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")  \n",
    "meteor = evaluate.load(\"meteor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BartForConditionalGeneration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m initial_model \u001b[38;5;241m=\u001b[39m \u001b[43mBartForConditionalGeneration\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlucadiliello/bart-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m finetuned_model \u001b[38;5;241m=\u001b[39m BartForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./bart_summarizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m finetuned_tokenizer \u001b[38;5;241m=\u001b[39m BartTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./bart_summarizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BartForConditionalGeneration' is not defined"
     ]
    }
   ],
   "source": [
    "initial_model = BartForConditionalGeneration.from_pretrained(\"lucadiliello/bart-small\")\n",
    "\n",
    "finetuned_model = BartForConditionalGeneration.from_pretrained(\"./bart_summarizer\")\n",
    "finetuned_tokenizer = BartTokenizer.from_pretrained(\"./bart_summarizer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Model Metrics:\n",
      "BLEU (Initial pre-trained model): 0.0090\n",
      "METEOR (Initial pre-trained model): 0.1610\n",
      "\n",
      "Fine-tuned Model Metrics:\n",
      "BLEU (Fine-tuned model): 0.0000\n",
      "METEOR (Fine-tuned model): 0.0715\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model_to_eval, dataset, tokenizer, max_input_length=512, max_target_length=64):\n",
    "    model_to_eval.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for example in dataset.select(range(200)):\n",
    "        input_text = \"summarize: \" + example[\"input_text\"]\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True, max_length=max_input_length)\n",
    "        input_ids = input_ids.to(model_to_eval.device)\n",
    "        with torch.no_grad():\n",
    "            output_ids = model_to_eval.generate(input_ids, max_length=max_target_length)\n",
    "        pred = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        predictions.append(pred)\n",
    "        references.append(example[\"target_text\"])\n",
    "    \n",
    "    results = {}\n",
    "    # ROUGE-L\n",
    "    #rouge_results = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "    #results[\"rougeL\"] = rouge_results[\"rougeL\"]\n",
    "    # BLEU score\n",
    "    bleu_results = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    results[\"bleu\"] = bleu_results[\"bleu\"]\n",
    "    # METEOR score\n",
    "    meteor_results = meteor.compute(predictions=predictions, references=references)\n",
    "    results[\"meteor\"] = meteor_results[\"meteor\"]\n",
    "\n",
    "    return results\n",
    "\n",
    "# Initial model\n",
    "initial_results = evaluate_model(initial_model, eval_dataset, tokenizer)\n",
    "print(\"Initial Model Metrics:\")\n",
    "#print(f\"ROUGE-L (Initial pre-trained model): {initial_results['rougeL']:.4f}\")\n",
    "print(f\"BLEU (Initial pre-trained model): {initial_results['bleu']:.4f}\")\n",
    "print(f\"METEOR (Initial pre-trained model): {initial_results['meteor']:.4f}\")\n",
    "\n",
    "# Fine-tuned model\n",
    "finetuned_results = evaluate_model(finetuned_model, eval_dataset, tokenizer)\n",
    "print(\"\\nFine-tuned Model Metrics:\")\n",
    "#print(f\"ROUGE-L (Fine-tuned model): {finetuned_results['rougeL']:.4f}\")\n",
    "print(f\"BLEU (Fine-tuned model): {finetuned_results['bleu']:.4f}\")\n",
    "print(f\"METEOR (Fine-tuned model): {finetuned_results['meteor']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'finetuned_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis Item Taste Like Dirt.. I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve Prob Used it 4 Times & Now It\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Just Sitting in MY Freezer.. I Have A High Tolerance for Nasty Stuff.. Just Don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt Really Like this Product.. Something In Grinding It Up Makes It Taste Nasty.. The Hulled Seeds Nutiva Sells Are Way Better.. If You Want Good Tasting Hemp Protein Powder It\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms $15/lb @ Earthshiftproducts.com but It Taste Wayyy Better Actually Taste Good From Earthshift..\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Tokenize the input\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfinetuned_tokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mencode(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Generate summary\u001b[39;00m\n\u001b[1;32m     11\u001b[0m summary_ids \u001b[38;5;241m=\u001b[39m finetuned_model\u001b[38;5;241m.\u001b[39mgenerate(inputs, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'finetuned_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Load your fine-tuned model and tokenizer\n",
    "#tokenizer = BartTokenizer.from_pretrained(\"path_to_your_fine_tuned_model\")\n",
    "#model = BartForConditionalGeneration.from_pretrained(\"path_to_your_fine_tuned_model\")\n",
    "\n",
    "# Sample input text\n",
    "input_text = \"This Item Taste Like Dirt.. I've Prob Used it 4 Times & Now It's Just Sitting in MY Freezer.. I Have A High Tolerance for Nasty Stuff.. Just Don't Really Like this Product.. Something In Grinding It Up Makes It Taste Nasty.. The Hulled Seeds Nutiva Sells Are Way Better.. If You Want Good Tasting Hemp Protein Powder It's $15/lb @ Earthshiftproducts.com but It Taste Wayyy Better Actually Taste Good From Earthshift..\"\n",
    "# Tokenize the input\n",
    "inputs = finetuned_tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Generate summary\n",
    "summary_ids = finetuned_model.generate(inputs, max_length=64, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Decode and print the summary\n",
    "summary = finetuned_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_outputs(initial_model, finetuned_model, dataset, tokenizer, device, sample_size=5, max_input_length=512, max_target_length=64):\n",
    "\n",
    "    indices = list(range(len(dataset)))\n",
    "    random_indices = random.sample(indices, sample_size)\n",
    "    \n",
    "    for idx in random_indices:\n",
    "        example = dataset[idx]\n",
    "        input_str = \"summarize: \" + example[\"input_text\"]\n",
    "        input_ids = tokenizer.encode(input_str, return_tensors=\"pt\",\n",
    "                                     truncation=True, max_length=max_input_length).to(device)\n",
    "        \n",
    "        # Initial model\n",
    "        with torch.no_grad():\n",
    "            initial_output_ids = initial_model.generate(input_ids, max_length=max_target_length)\n",
    "        initial_output = tokenizer.decode(initial_output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Fine-tuned model\n",
    "        with torch.no_grad():\n",
    "            finetuned_output_ids = finetuned_model.generate(input_ids, max_length=max_target_length)\n",
    "        finetuned_output = tokenizer.decode(finetuned_output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Display the outputs\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Example ID: {idx}\")\n",
    "        print(\"Input Text:\")\n",
    "        print(example[\"input_text\"])\n",
    "        print(\"\\nReference Summary:\")\n",
    "        print(example[\"target_text\"])\n",
    "        print(\"\\nInitial Model Output:\")\n",
    "        print(initial_output)\n",
    "        print(\"\\nFine-tuned Model Output:\")\n",
    "        print(finetuned_output)\n",
    "        print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sample_outputs(initial_model, model, eval_dataset, tokenizer, device, sample_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
