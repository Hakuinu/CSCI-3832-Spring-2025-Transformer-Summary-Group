{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import random\n",
    "from evaluate import load\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "import kagglehub\n",
    "import evaluate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version of amazon fine food reviews dataset\n",
    "path = kagglehub.dataset_download(\"snap/amazon-fine-food-reviews\")\n",
    "print(\"Path to dataset files:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Reviews.csv\", usecols=[\"Id\",\"Summary\", \"Text\", \"ProductId\"])\n",
    "df.dropna(subset=[\"Summary\", \"Text\"], inplace=True)\n",
    "df = df.sample(20000, random_state=42)\n",
    "df = df.rename(columns={\"Summary\": \"target_text\", \"Text\": \"input_text\"})\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(len(dataset))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(\"lucadiliello/bart-small\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"lucadiliello/bart-small\")\n",
    "\n",
    "print(len(dataset))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    ) | {\n",
    "        \"labels\": tokenizer(\n",
    "            examples[\"target_text\"],\n",
    "            max_length=64,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )[\"input_ids\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split[\"train\"]\n",
    "eval_dataset = split[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train = train_dataset#.select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir= \"./bart_summarizer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir= \"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./bart_summarizer\")\n",
    "tokenizer.save_pretrained(\"./bart_summarizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels will be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "initial_model = BartForConditionalGeneration.from_pretrained(\"lucadiliello/bart-small\")\n",
    "\n",
    "finetuned_model = BartForConditionalGeneration.from_pretrained(\"./bart_summarizer\")\n",
    "finetuned_tokenizer = BartTokenizer.from_pretrained(\"./bart_summarizer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Model Metrics:\n",
      "ROUGE-L (Initial pre-trained model): 0.0711\n",
      "\n",
      "Fine-tuned Model Metrics:\n",
      "ROUGE-L (Fine-tuned model): 0.1335\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model_to_eval, dataset, tokenizer, max_input_length=512, max_target_length=64):\n",
    "    model_to_eval.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for example in dataset.select(range(200)):\n",
    "        input_text = \"summarize: \" + example[\"input_text\"]\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True, max_length=max_input_length)\n",
    "        input_ids = input_ids.to(model_to_eval.device)\n",
    "        with torch.no_grad():\n",
    "            output_ids = model_to_eval.generate(input_ids, max_length=max_target_length)\n",
    "        pred = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        predictions.append(pred)\n",
    "        references.append(example[\"target_text\"])\n",
    "    \n",
    "    results = {}\n",
    "    # ROUGE-L\n",
    "    rouge_results = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "    results[\"rougeL\"] = rouge_results[\"rougeL\"]\n",
    "\n",
    "    return results\n",
    "\n",
    "# Initial model\n",
    "initial_results = evaluate_model(initial_model, eval_dataset, tokenizer)\n",
    "print(\"Initial Model Metrics:\")\n",
    "print(f\"ROUGE-L (Initial pre-trained model): {initial_results['rougeL']:.4f}\")\n",
    "\n",
    "# Fine-tuned model\n",
    "finetuned_results = evaluate_model(finetuned_model, eval_dataset, tokenizer)\n",
    "print(\"\\nFine-tuned Model Metrics:\")\n",
    "print(f\"ROUGE-L (Fine-tuned model): {finetuned_results['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: Great cinnamon bears\n"
     ]
    }
   ],
   "source": [
    "input_text =\"These cinnamon bears have great flavor and do not taste sugar free.  My only issue is that they should be softer.\"\n",
    "inputs = finetuned_tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "summary_ids = finetuned_model.generate(inputs, max_length=64, num_beams=4, early_stopping=True)\n",
    "\n",
    "summary = finetuned_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Id', 'ProductId', 'target_text', 'input_text', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 2000\n",
      "})\n",
      "\n",
      "Input: summarize: I thought that these bars were very tasty if a bit on the expensive side.  My main complaint is that the bars disintegrate as you open up the packaging. Since the bars are also very sticky remain stuck to the rapper as they break apart so you are stuck having to pick off pieces of the bar from the inside of the wrapper (which is difficult because the wrapper is plastic not foil). I don't know if I got an old box of these or if they're all like this. Regardless I probably won't risk getting another box like the first.\n",
      "Generated Summary: delicious\n",
      "\n",
      "Input: summarize: This cat food is great. My big tom cat eats it as well as my two tiny doggies. Its all natural and doesnt make them sick or throw up. Nothing like that. That animals normally do with cheap processed pet food. And Amazon's price is much cheaper than the health food stores around. Good luck!\n",
      "Generated Summary: great cat food\n",
      "\n",
      "Input: summarize: I was so excited to buy these fruit bars because they are just so good. I ordered this KIND PLUS Almond Cashew PACK OF 12 and guess what? When I openned the package this morning I was so confused if there was anything wrong... Instead of 12 fruit bars, there were only 8!!!! I shop at Amazon all the time and never complain or anything but this is so RIDICULOUS! I couldn't believe my eyes I just thought that I was wrong. But I keep counting over and over and there were just 8! Now I don't want to say anything but so mad!\n",
      "Generated Summary: YUMMY!\n",
      "\n",
      "Input: summarize: I used to wait until the end of summer, buy a bushel of tomatoes and can all my spaghetti sauce at once using a similar but bulk product. It wasn't as tasty as this and had lots of MSG in it. It also required me to do all my canning at once. This year, I've been able to do a small batch of sauce every week with my own tomatoes as they ripen in about 15 minutes. I alter the instructions on the packet to use 29 ounces of fresh tomatoes lightly pureed in the food processor. Dump into a saucepan with one tablespoon balsamic vinegar and one tablespoon of oil, bring to a boil, simmer 10-15 minutes (depending on how watery the tomatoes are) or until it is the consistency you like your sauce to be. Each batch makes about the equivalent of a regular jar of spaghetti sauce (25 to 28 ounces?). I've easily got enough sauce stored away now for winter and to give away (I've been freezing but you could easily water bath process this for shelf storage). I have used for pizza sauce and pasta sauce as well as added a little to soups and used for a dip with zucchini bites etc. Doubles well - I haven't tried more than that at one use. The mix dissolves evenly without clumping. I like that the packaging is not excessive, too. Easily the most indispensable thing I have had in my kitchen this summer.\n",
      "Generated Summary: good stuff\n",
      "\n",
      "Input: summarize: When I first looked at this bonsai and the pictures of what other people had received, I thought this would be a great purchase. However, I bought the $6.95 tree from 9GreenBox instead of the $12.48 tree from Bonsai Boy. I wish I had spent the extra money. The tree cannot be more than a season and a half old (while the description says that it is a 3-year-old tree), and is about 3 1/2\" - 4\" tall. That being said, there is plenty of new growth and it was packaged well (no breakage and soil was still wet) and I received it quicker than I was expecting.\n",
      "Generated Summary: good value\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(eval_dataset)\n",
    "indices = list(range(len(eval_dataset)))\n",
    "random_indices = random.sample(indices, 5)\n",
    "\n",
    "for idx in random_indices:\n",
    "    example = eval_dataset[idx]  \n",
    "\n",
    "    # build the prompt string\n",
    "    input_str = \"summarize: \" + example[\"input_text\"]\n",
    "    print(\"\\nInput:\", input_str)\n",
    "\n",
    "    inputs = finetuned_tokenizer(\n",
    "        input_str,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # 5) generate and decode the first element\n",
    "    summary_ids = finetuned_model.generate(\n",
    "        **inputs,\n",
    "        max_length=64,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    summary = finetuned_tokenizer.decode(\n",
    "        summary_ids[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(\"Generated Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
